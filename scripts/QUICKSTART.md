# Custom Inference Quick Start

## Where to Run

**Always run from the project root:**

```bash
cd /path/to/Ryze-Data
python scripts/run_custom_inference.py --dataset arxivqa --experiment baseline ...
```

All default paths (`--mapping-dir`, `--pdf-dir`, `--ocr-root`, `--output-dir`) are relative
to the working directory, so the script expects `cwd = Ryze-Data/`.

## Data Layout

Place all data under the `data/` directory at the project root. The full expected tree:

```
Ryze-Data/                              ← run scripts from here
└── data/
    ├── id_mappings/                    ← QA metadata (required)
    │   ├── arxivqa_id_mapping.json
    │   └── slidevqa_id_mapping.json
    │
    ├── benchmark_data/
    │   ├── ocr_pdfs/                   ← source PDFs (required)
    │   │   ├── arxivqa/
    │   │   │   ├── arxivqa_0.pdf
    │   │   │   ├── arxivqa_1.pdf
    │   │   │   └── ...
    │   │   └── slidevqa/
    │   │       ├── slidevqa_0.pdf
    │   │       ├── slidevqa_1.pdf
    │   │       └── ...
    │   └── inference_images/           ← auto-generated PNG cache (created at runtime)
    │       ├── arxivqa/
    │       └── slidevqa/
    │
    ├── ocr_precompute/                 ← OCR results (required for non-baseline experiments)
    │   ├── deepseek_ocr_v1/            ← experiment "baseline1"
    │   │   ├── arxivqa/
    │   │   │   ├── arxivqa_0/
    │   │   │   │   ├── arxivqa_0.md    ← primary OCR markdown
    │   │   │   │   └── result.mmd      ← alternative name (also detected)
    │   │   │   ├── arxivqa_1/
    │   │   │   │   └── arxivqa_1.md
    │   │   │   └── ...
    │   │   └── slidevqa/
    │   │       └── ...                 ← same structure
    │   ├── deepseek_ocr_v2/            ← experiment "baseline2"  (same layout)
    │   ├── markitdown/                 ← experiment "baseline3"  (same layout)
    │   └── marker/                     ← experiment "us"         (same layout)
    │
    └── custom_inference/               ← output results (created at runtime)
        ├── arxivqa/
        │   ├── baseline/
        │   │   └── results.jsonl
        │   ├── baseline1/
        │   │   └── results.jsonl
        │   └── ...
        └── slidevqa/
            └── ...
```

### Where each file comes from

| Directory | What goes here | How to get it |
|---|---|---|
| `data/id_mappings/` | QA metadata JSONs (questions, options, labels) | Generated by `scripts/utils/benchmark/id_mapping_generator.py` |
| `data/benchmark_data/ocr_pdfs/{dataset}/` | One PDF per sample (`{sample_id}.pdf`) | Generated from dataset images via the benchmark pipeline |
| `data/ocr_precompute/{model}/{dataset}/` | OCR markdown output, one subdir per sample | Generated by `scripts/precompute_ocr.py` |
| `data/benchmark_data/inference_images/` | Rendered PNGs from PDFs | Auto-created on first run (cached) |
| `data/custom_inference/` | Inference results | Auto-created by this script |

### OCR markdown resolution order

For each sample, the script looks for OCR output in this order:

1. `{ocr_dir}/{sample_id}/{sample_id}.md`
2. `{ocr_dir}/{sample_id}/result.mmd`
3. `{ocr_dir}/{sample_id}.md`
4. Any `*.md` file inside `{ocr_dir}/{sample_id}/`

## Prerequisites

1. **vLLM server** running with Qwen3-VL-8B (or compatible model):
   ```bash
   # Example: start vLLM on GPU 2, port 8000
   CUDA_VISIBLE_DEVICES=2 vllm serve Qwen3-VL-8B-Instruct \
       --port 8000 --max-model-len 8192
   ```

2. **Data files** in place (see Data Layout above).

## Experiments & OCR Methods

| Experiment | OCR Method | Description | OCR Precompute Dir |
|---|---|---|---|
| `baseline` | *(none)* | Vision-only — image sent directly to VLM, no OCR | N/A |
| `baseline1` | DeepSeek-OCR v1 | First-generation DeepSeek OCR model | `data/ocr_precompute/deepseek_ocr_v1/{dataset}/` |
| `baseline2` | DeepSeek-OCR v2 | Second-generation DeepSeek OCR model | `data/ocr_precompute/deepseek_ocr_v2/{dataset}/` |
| `baseline3` | MarkItDown | Microsoft MarkItDown converter | `data/ocr_precompute/markitdown/{dataset}/` |
| `us` | Marker | Marker PDF-to-markdown pipeline | `data/ocr_precompute/marker/{dataset}/` |

**How it works:** For `baseline`, only the rendered PDF page image(s) are sent to the VLM.
For all other experiments, the corresponding precomputed OCR markdown is loaded from disk
and included in the prompt alongside the image, giving the model both visual and textual context.

## Usage

All commands below assume `cwd = Ryze-Data/`.

### Baseline (vision-only, no OCR)

```bash
# ArxivQA — outputs JSON structured answers, e.g. {"answer": "B"}
python scripts/run_custom_inference.py \
    --dataset arxivqa \
    --experiment baseline \
    --endpoints http://localhost:8000/v1 \
    --max-samples 50

# SlideVQA — outputs concise free-text answers
python scripts/run_custom_inference.py \
    --dataset slidevqa \
    --experiment baseline \
    --endpoints http://localhost:8000/v1 \
    --max-samples 50
```

### OCR-augmented experiments

```bash
# DeepSeek-OCR v1
python scripts/run_custom_inference.py \
    --dataset arxivqa \
    --experiment baseline1 \
    --endpoints http://localhost:8000/v1

# DeepSeek-OCR v2
python scripts/run_custom_inference.py \
    --dataset arxivqa \
    --experiment baseline2 \
    --endpoints http://localhost:8000/v1

# MarkItDown
python scripts/run_custom_inference.py \
    --dataset slidevqa \
    --experiment baseline3 \
    --endpoints http://localhost:8000/v1

# Marker (allow missing OCR files — skip instead of error)
python scripts/run_custom_inference.py \
    --dataset slidevqa \
    --experiment us \
    --endpoints http://localhost:8000/v1 \
    --allow-missing-ocr
```

### Multiple endpoints (load balancing)

```bash
python scripts/run_custom_inference.py \
    --dataset arxivqa \
    --experiment baseline \
    --endpoints http://gpu1:8000/v1,http://gpu2:8000/v1 \
    --concurrency 32
```

### Using a public API (OpenAI-compatible)

The script works with any OpenAI-compatible endpoint. Pass the API key via
`--api-key` or set an environment variable (`$OPENAI_API_KEY` or `$API_KEY`).

```bash
# Option 1: pass key directly
python scripts/run_custom_inference.py \
    --dataset arxivqa \
    --experiment baseline \
    --endpoints https://api.openai.com/v1 \
    --model gpt-4o \
    --api-key sk-xxxxxxxxxxxx

# Option 2: use environment variable (recommended — keeps key out of shell history)
export OPENAI_API_KEY="sk-xxxxxxxxxxxx"
python scripts/run_custom_inference.py \
    --dataset arxivqa \
    --experiment baseline \
    --endpoints https://api.openai.com/v1 \
    --model gpt-4o

# Option 3: other OpenAI-compatible providers
export API_KEY="your-key-here"
python scripts/run_custom_inference.py \
    --dataset slidevqa \
    --experiment baseline \
    --endpoints https://api.deepseek.com/v1 \
    --model deepseek-chat

# SiliconFlow, Together AI, etc. — same pattern
python scripts/run_custom_inference.py \
    --dataset arxivqa \
    --experiment baseline \
    --endpoints https://api.siliconflow.cn/v1 \
    --model Qwen/Qwen2.5-VL-72B-Instruct \
    --api-key "$SILICONFLOW_API_KEY"
```

**API key resolution order:** `--api-key` flag > `$OPENAI_API_KEY` > `$API_KEY` > `"EMPTY"` (local vLLM).

## Key Options

| Flag | Default | Description |
|------|---------|-------------|
| `--dataset` | *(required)* | `arxivqa` or `slidevqa` |
| `--experiment` | *(required)* | `baseline`, `baseline1`, `baseline2`, `baseline3`, `us` |
| `--endpoints` | `localhost:8000/v1` | Comma-separated OpenAI-compatible API base URLs |
| `--model` | `Qwen3-VL-8B` | Model name passed in API requests |
| `--api-key` | `$OPENAI_API_KEY` / `$API_KEY` / `EMPTY` | API key for authentication |
| `--max-samples` | `0` (all) | Limit number of samples to process |
| `--concurrency` | `16` | Parallel request threads |
| `--max-tokens` | `512` | Fallback max tokens (ArxivQA overrides to 32) |
| `--temperature` | `0.0` | Sampling temperature |
| `--timeout` | `120.0` | Per-request timeout in seconds |
| `--max-retries` | `2` | Retries on failure (cycles through endpoints) |
| `--allow-missing-ocr` | off | Skip samples with missing OCR instead of failing |

## Output

Results are written to `data/custom_inference/{dataset}/{experiment}/results.jsonl`:

```jsonl
{"sample_id": "arxivqa_0", "answer": "B", "raw_answer": "{\"answer\": \"B\"}", "reference": "B", "question_type": "multiple_choice"}
{"sample_id": "slidevqa_5", "answer": "42%", "raw_answer": "42%", "reference": "42%", "question_type": "free_text"}
```

- `answer` — post-processed result used for evaluation
- `raw_answer` — verbatim model output before post-processing
- **ArxivQA**: both `answer` and `reference` are normalized to a single letter (A/B/C/D)
- **SlideVQA**: raw text preserved as-is
- Results are append-only with automatic resume — rerun the same command to continue from where it stopped

## Customization

Edit `process_sample()` in `run_custom_inference.py` to change prompt logic. The function receives all sample fields and returns an `InferenceRequest` with messages, optional `max_tokens`, and optional `response_format`.
